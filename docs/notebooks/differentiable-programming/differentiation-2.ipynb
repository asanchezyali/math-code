{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation\n",
    "**Author:** Alejandro Sánchez Yalí\n",
    "\n",
    "In this chapter, we review key differentiation concepts. In particular, we emphasize on the fundamental role played by\n",
    "linear approximations in the context of numerical differentiation. We also discuss the concept of automatic\n",
    "differentiation, which is a powerful tool for computing derivatives of functions implemented in computer programs.\n",
    "\n",
    "## Univariate differentiation\n",
    "### Derivatives\n",
    "Before studying derivatives, we recall the definition of function continuity.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.1. Continuous function</p> \n",
    "  A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous at a point $x_0$ if $$\\lim_{x \\to x_0} f(x) = f(x_0).$$\n",
    "  A function $f$ is continuous if it is continuous at every point in its domain.\n",
    "</div>\n",
    "\n",
    "In the following, we use Landau's notation to describe the behavior of functions near a point. We write $$f(x) = o\\big(g(x)\\big) \\quad \\text{as} \\quad x \\to x_0$$\n",
    "if $$\\lim_{x \\to x_0} \\frac{|f(x)|}{|g(x)|} = 0.$$\n",
    "\n",
    "That is, $f(x)$ is much smaller than $g(x)$ as $x$ approaches $x_0.$ For example, $f$ is continuous at $x_0$ \n",
    "if $$f(x_0 + \\delta) = f(x_0) + o(1) \\quad \\text{as} \\quad \\delta \\to 0.$$\n",
    "\n",
    "We now introduce the concept of derivative. Consider a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ and a point\n",
    "$x_0$ in its domain. Its value on an interval $[x_0, x_0 + h]$ can be approximated by the secant between $\\big(x_0, f(x_0)\\big)$\n",
    "and $\\big(x_0 + h, f(x_0 + h)\\big)$. The slope of this **secant** is given by the difference quotient $$\\frac{f(x_0 + h) -\n",
    "f(x_0)}{h}.$$ In the limit of an infinitesimal $h$, the secant converges to the **tangent** at $\\big(x_0, f(x_0)\\big)$. The slope\n",
    "of this tangent is the derivative of $f$ at $x_0$, denoted by $f'(x_0).$ The definition below\n",
    "formalizes this intuition.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.2. Derivative</p>\n",
    "  The derivative of a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ at a point $x_0$ is defined as \n",
    "  $$f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}.$$ If $f'(x_0)$ is well-defined at a particular $x_0$, \n",
    "  we say that the function $f$ is differentiable at $x_0$.\n",
    "</div>\n",
    "\n",
    "Here, and in the following definitions, if $f$ is differentiable at any $x$, we say that it is **differentiable\n",
    "everywhere** of simply **differentiable**. If $f$ is differentiable at a given $x$, then it is necessarily continuous at\n",
    "$x$.\n",
    "\n",
    "<div class=\"theorem\"><p>Theorem 1.1. Differentiability implies continuity</p> \n",
    "  If a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is differentiable at a point $x_0$, then it is continuous at $x_0$.\n",
    "</div>\n",
    "\n",
    "*Proof.* The proof follows from the definition of derivative. We have $$f(x_0 + h) = f(x_0) + f'(x_0)h + o(h) \\quad\n",
    "\\text{as} \\quad h \\to 0.$$ Since $f'(x_0)h + o(h) = o(1)$ as $h \\to 0$, we have that  $$\\lim_{h \\to 0} |f(x_0 + h) -\n",
    "f(x_0)| = 0.$$ Therefore, $f$ is continuous at $x_0$.\n",
    "\n",
    "In addition to enabling the computation of the slope of a function at a point, the derivative provides information about\n",
    "the **mononicity** of $f$ near that point. For example, if $f'(x_0) > 0$, then $f$ is increasing near $x_0$. If $f'(x_0) < 0$,\n",
    "then $f$ is decreasing near $x_0$. If $f'(x_0) = 0$, then $f$ has a local extremum at $x_0$. Such information can be\n",
    "used to develop iterative algorithms to minimize or maximize $f$ by computing iterates of the form $$x_{n+1} = x_n - \\alpha f'(x_n),$$\n",
    "where $\\alpha$ is a step size. If $\\alpha > 0$, the algorithm converges to a local minimum of $f$. If $\\alpha < 0,$ it\n",
    "converges to a local maximum. If $f$ is convex, the algorithm converges to the global minimum. \n",
    "\n",
    "For several elementary functions, the derivative can be computed analytically. \n",
    "\n",
    "<div class=\"example\"><p>Example 1.1. Derivative of power function</p> \n",
    "  The derivative of $f(x) = x^n$ for $x \\in \\mathbb{R}$ and $n \\in \\mathbb{N}\\setminus \\{0\\}$ is given by $f'(x) = nx^{n-1}$. In fact, we consider $f(x) = x^n$ for $x \\in \\mathbb{R}$ and $n \\in \\mathbb{N}\\setminus \\{0\\}$. We have $$\\begin{equation}f(x + h) = (x + h)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} h^k\\end{equation}$$ Therefore, $$\\begin{equation}\\begin{split}f(x + h) - f(x) & = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} h^k - x^n \\\\ & = \\sum_{k=1}^n \\binom{n}{k} x^{n-k} h^k.\\end{split}\\end{equation}$$ Dividing by $h$ and taking the limit as $h \\to 0$, we obtain $$\\begin{equation}\\begin{split}f'(x) & = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} \\\\ & = \\lim_{h \\to 0} \\sum_{k=1}^n \\binom{n}{k} x^{n-k} h^{k-1} \\\\ & = nx^{n-1}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "<div class=\"remark\"><p>Remark 1.1. Functions on a subset $U$ of $\\mathbb{R}$</p> \n",
    "  For simplicity, we consider functions $f: \\mathbb{R} \\rightarrow \\mathbb{R}.$ However, the concept of derivative can be extended to functions defined on a subset $U$ of $\\mathbb{R}.$ If a fuction $f: U \\rightarrow \\mathbb{R}$ is defined on a subset $U$ of $\\mathbb{R}$, as it is the case for $f(x) = \\sqrt{x}$, defined on $U=\\mathbb{R}^+,$ the derivative of $f$ at a point $x_0 \\in U$ is defined on a neighborhood of $x_0$ contained in $U$, that is, there exist $r > 0$ such that $f$ is differentiable on $x_0 + \\epsilon \\in U$ for all $|\\epsilon| < r.$ The function $f$ is then said **differentiable everywhere** or differentiable for short if it is differentiable at every point in the **interior** of $U$, the set of points in $U$ such that $\\{x + \\epsilon: |\\epsilon| < r\\} \\subset U$ for some $r > 0$. For points lying at the boundary of $U$, the concept of derivative is more subtle and requires the definition of **one-sided derivatives**, meaning that the limit in the definition of derivative is taken from the left or from the right. For example, the derivative of $f(x) = \\sqrt{x}$ at $x=0$ is not defined, since the function is not defined for negative values of $x$. However, the derivative of $f(x) = \\sqrt{x}$ at $x=0$ is defined from the right, and it is equal to $1/2.$\n",
    "</div>\n",
    "\n",
    "### Calculus rules\n",
    "\n",
    "For a given $x \\in \\mathbb{R}$ and two functions $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to \\mathbb{R},$ the\n",
    "derivative of elementary operations on $f$ and $g$ such as their sums, products or compositions can easily be derived\n",
    "from the definition of the derivative, under appropriate conditions on the differentiability properties of $f$ and $g$\n",
    "at $x$. For example, if the functions $f$ and $g$ are differentiable at $x,$ then the sum $af + bg$ and the product $fg$\n",
    "are differentiable at $x$ for any $a, b \\in \\mathbb{R}$, and their derivatives are given by \n",
    "\n",
    "1. Linearity: $(af + bg)'(x) = af'(x) + bg'(x).$\n",
    " \n",
    "2. Product rule: $(fg)'(x) = f'(x)g(x) + f(x)g'(x),$ where $(fg)(x) + f(x)g(x).$\n",
    "\n",
    "The linearity can be verified directly from the linearity of the limit operator. For the product rule, we have\n",
    "\n",
    "<div class=\"non-display-mobile\">\n",
    "$$\\begin{equation}\\begin{split} \\frac{(fg)(x + h) - (fg)(x)}{h} & = \\frac{f(x+h)g(x + h) - \\color{red}{f(x)g(x + h)}}{h} \\\\ & + \\frac{{\\color{red}f(x)g(x + h)} - fg(x)}{h} \\\\ & = g(x + h) \\frac{f(x + h) - f(x)}{h} \\\\ & + f(x)\n",
    "\\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"non-display-desktop\">\n",
    "$$\\begin{equation}\\begin{split} & \\frac{(fg)(x + h) - (fg)(x)}{h} = \\\\ & = \\frac{f(x+h)g(x + h) - \\color{red}{f(x)g(x + h)}}{h} \\\\ & + \\frac{{\\color{red}f(x)g(x + h)} - f(x)g(x)}{h} \\\\ & = g(x + h) \\frac{f(x + h) - f(x)}{h} \\\\ & + f(x)\n",
    "\\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "If the derivatives of $g$ at $x$ and of $f$ at $g(x)$ exist, then the derivative of the composition $(f\\circ g)(x) =\n",
    "f(g(x))$ at $x$ is given by the **chain rule**:\n",
    "\n",
    "3. Chain rule: $(f\\circ g)'(x) = f'(g(x))g'(x).$\n",
    "\n",
    "The chain rule can be derived by considering the limit of the difference quotient of the composition $(f\\circ g)(x)$ as\n",
    "$h \\to 0$:\n",
    "\n",
    "<div class=\"non-display-mobile\">\n",
    "$$\\begin{equation}\\begin{split} \\frac{(f\\circ g)(x + h) - (f\\circ g)(x)}{h} & = \\frac{f(g(x + h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)} \\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"non-display-desktop\">\n",
    "$$\\begin{equation}\\begin{split} & \\frac{(f\\circ g)(x + h) - (f\\circ g)(x)}{h} = \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)} \\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "As seen in the sequel, the linearity and the product rule can be seen as byproducts of the chain rule, making the chain\n",
    "rule the cornerstone of differentiation.\n",
    "\n",
    "For now, consider a function that can be expressed as sums, products aor compositions of elementary functions such as\n",
    "$f(x) = \\exp(x) \\ln(x) + \\cos x^2.$ Its derivative can be computed by applying the aforementioned rules on the\n",
    "decomposition of $f$ into elementary operations and functions. \n",
    "\n",
    "\n",
    "<div class=\"example\"><p>Example 1.2. Applying rules of differentiation</p> \n",
    "Consider the function $f(x) = \\exp(x) \\ln(x) + \\cos x^2.$ We have $$f'(x) = \\exp(x) \\ln(x) + \\exp(x) \\frac{1}{x} - 2x \\sin x^2.$$ The derivative of $f$ on $x > 0$ can be computed step by step as follows, denoting $\\operatorname*{sq}(x) := x^2$,\n",
    "$$\\begin{equation}\\begin{split}f'(x) &  = (\\exp \\cdot \\ln)'(x) + (\\cos \\circ \\operatorname*{sq})'(x) \\\\ & = \\exp'(x) \\ln(x) + \\exp(x) \\ln'(x) + \\cos'(\\operatorname*{sq}(x)) \\operatorname*{sq}'(x) \\\\ & = \\exp(x) \\ln(x) + \\exp(x) \\frac{1}{x} - 2x \\sin x^2.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "### Leibniz's notation\n",
    "\n",
    "The notion of derivative was first introduced independently by Isaac Newton and Gottfried Wilhelm Leibniz in the 18th\n",
    "century. The latter considered derivatives as the quotient of infinitesimals variations. Namely, denoting $y = f(x)$ a\n",
    "variable depending on $x$ through $f$, Leibniz considered the derivative of $f$ as the quotient:\n",
    "\n",
    "$$f' = \\frac{dy}{dx} \\quad \\text{with} \\quad f'(x) = \\left. \\frac{df}{dx}\\\\\\right|_{x}.$$\n",
    "\n",
    "where $dy$ and $dx$ are infinitesimal variations of $y$ and $x$, respectively and the symbol $\\left.\n",
    "\\frac{df}{dx}\\\\\\right|_{x}$ denotes the derivative of $f$ at $x$. The notation $\\frac{dy}{dx}$ is called **Leibniz's\n",
    "notation**. It is particularly useful to express the chain rule, as it allows to express the derivative of a composition\n",
    "of functions as the product of the derivatives of the functions involved in the composition. If we have for $z = g(y)$\n",
    "and $y = f(x)$, then the chain rule can be expressed as:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}.$$\n",
    "\n",
    "This hints that derivatives are multiplied when considering compositions of functions. At evaluation, the chain rule in\n",
    "Leibniz notation recovers the formula presented above as\n",
    "\n",
    "$$\\left. \\frac{dz}{dx}\\\\\\right|_{x} = \\left. \\frac{dz}{dy}\\\\\\right|_{y} \\left. \\frac{dy}{dx}\\\\\\right|_{x} =\n",
    "g'(f(x))f'(x) = (g\\circ f)'(x).$$\n",
    "\n",
    "\n",
    "## Multivariate functions\n",
    "### Directional derivatives\n",
    "Let us now consider a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ of multiple inputs $x = (x_1, \\ldots, x_n) \\in\n",
    "\\mathbb{R}^n$. The most important example in machibne learning is a functions which, to the parameters\n",
    "$x\\in\\mathbb{R}^n$ of a neural network, associates the loss value in $\\mathbb{R}$. Variations of $f$ need to be defined\n",
    "along specific directions in $\\mathbb{R}^n$, such as the variation $f(x + \\delta v) - f(x)$ of $f$ around\n",
    "$x\\in\\mathbb{R}^n$ in the direction $v\\in\\mathbb{R}^n$. This consideration naturally leads to the definition of the\n",
    "directional derivative of $f$ at $x$ in the direction $v$.\n",
    "\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.3. Directional derivative</p>\n",
    "  The directional derivative of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ at a point $x \\in \\mathbb{R}^n$ in the direction $v \\in \\mathbb{R}^n$ is defined as $$\\partial f(x)[v] = \\lim_{h \\to 0} \\frac{f(x + hv) - f(x)}{h}.$$ If the limit exists, we say that $f$ is differentiable at $x$ in the direction $v$.\n",
    "</div>\n",
    "\n",
    "One example of directional derivative consists in computing the derivative of a function $f$ at a point $x$ in any of\n",
    "the canonical directions $e_i$ of $\\mathbb{R}^n$, where $e_i$ is the vector with a $1$ at the $i$-th position and $0$.\n",
    "\n",
    "This allows us to define the notion of **partial derivatives**, denoted for $i = 1, \\ldots, n$ by $$\\partial_i f(x)\n",
    ":=\\partial f(x)[e_i] = \\lim_{h \\to 0} \\frac{f(x + he_i) - f(x)}{h}.$$ \n",
    "\n",
    "This also denoted in Leibniz's notation by $\\partial_i f(x) = \\frac{\\partial f}{\\partial x_i}(x)$  or $\\partial_i f(x)\n",
    "= \\partial_{x_i} f(x).$ By moving along only the $i$-th coordinate of the function, the partial derivative is akin to\n",
    "using the function $\\phi(x_i)=f(x_1, \\cdots, x_n)$ around $x_i$, letting all other coordinates fixed at their values $x_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
