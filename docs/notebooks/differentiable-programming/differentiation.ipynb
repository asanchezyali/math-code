{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferenciación\n",
    "\n",
    "En este sesión revisamos los conceptos claves de diferenciación. En particular, enfatizamos en el papel fundamental que desempeñan las aproximaciones lineales en el contexto de la diferenciación numérica. También discutimos el concepto de **diferenciación automática**, que es una herramienta poderosa para calcular derivadas de funciones implementadas en programas de computadora.\n",
    "\n",
    "## Derivadas de funciones univariables\n",
    "### Derivadas y continuidad\n",
    "\n",
    "Antes de estudiar las derivadas, recordamos la definición de continuidad de una función.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.1. Función continua</p> \n",
    "  Una función $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ es **continua en un punto** $x_0$ si $$\\lim_{x \\to x_0} f(x) = f(x_0).$$ Una función $f$ es **continua** si es continua en cada punto de su dominio.\n",
    "</div>\n",
    "\n",
    "En lo siguiente, usamos la notación de Landau para describir el comportamiento de las funciones cerca de un punto.\n",
    "Escribimos $$f(x) = o\\big(g(x)\\big) \\quad \\text{cuando} \\quad x \\to x_0$$ si $$\\lim_{x \\to x_0} \\frac{|f(x)|}{|g(x)|} = 0.$$\n",
    "\n",
    "Es decir, $f(x)$ es mucho más pequeño que $g(x)$ a medida que $x$ se acerca a $x_0.$ Por ejemplo, $f$ es continua en $x_0$ si $$f(x_0 + \\delta) = f(x_0) + o(1) \\quad \\text{cuando} \\quad \\delta \\to 0.$$\n",
    "\n",
    "Ahora introducimos el concepto de derivada. Consideremos una función $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ y un punto $x_0$ en su dominio. Su valor en un intervalo $[x_0, x_0 + h]$ puede aproximarse mediante la sencante entre $\\big(x_0, f(x_0)\\big)$ y $\\big(x_0 + h, f(x_0 + h)\\big)$. La pendiente de esta **secante** está dada por el cociente diferencial\n",
    "\n",
    "$$\\frac{f(x_0 + h) - f(x_0)}{h}.$$ \n",
    "\n",
    "En el limite de un infinitesimal $h$, la secante converge a la **tangente** en $\\big(x_0, f(x_0)\\big)$. La pendiente de esta tangente es la derivada de $f$ en $x_0$, denotada por $f'(x_0).$ La definición formal de la derivada es la siguiente.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.2. Derivada</p>\n",
    "  La **derivada** de una función $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ en un punto $x_0$ está definida como \n",
    "  $$f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}.$$ Si $f'(x_0)$ está bien definida en $x_0$, en particular, si el límite existe, decimos que la función $f$ es **diferenciable** en $x_0$.\n",
    "</div>\n",
    "\n",
    "Aquí, y en las definiciones siguientes, si $f$ es diferenciable en cualquier $x$, decimos que es **diferenciable en todas partes** o simplemente **diferenciable**. Si $f$ es diferenciable en un punto dado $x$, entonces es necesariamente continua en $x$.\n",
    "\n",
    "<div class=\"theorem\"><p>Teorema 1.1. Diferenciabilidad implica continuidad</p> \n",
    "  Si una función $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ es diferenciable en un punto $x_0$, entonces es continua en $x_0$.\n",
    "</div>\n",
    "\n",
    "**Demostración.** La prueba sigue de la definición de derivada. Tenemos $$f(x_0 + h) = f(x_0) + f'(x_0)h + o(h) \\quad \\text{cuando} \\quad h \\to 0.$$ Dado que $f'(x_0)h + o(h) = o(1)$ cuando $h \\to 0$, tenemos que $$\\lim_{h \\to 0} |f(x_0 + h) - f(x_0)| = 0.$$ Por lo tanto, $f$ es continua en $x_0$.\n",
    "\n",
    "Además de permitir el cálculo de la pendiente de una función en un punto, la derivada proporciona información sobre la **monotonía** de $f$ cerca de ese punto. Por ejemplo, si $f'(x_0) > 0$, entonces $f$ es creciente cerca de $x_0$. Si $f'(x_0) < 0$, entonces $f$ es decreciente cerca de $x_0$. Si $f'(x_0) = 0$, entonces $f$ tiene un extremo local en $x_0$. Esta información puede usarse para desarrollar algoritmos iterativos para minimizar o maximizar $f$ calculando iterados de la forma $$x_{n+1} = x_n - \\alpha f'(x_n),$$ donde $\\alpha$ es un tamaño de paso. Si $\\alpha > 0$, el algoritmo converge a un mínimo local de $f$. Si $\\alpha < 0,$ converge a un máximo local. Si $f$ es convexa, el algoritmo converge al mínimo global.\n",
    "\n",
    "Para varias funciones elementales, la derivada se puede calcular analíticamente.\n",
    "\n",
    "<div class=\"example\"><p>Ejemplo 1.1. Derivada de una función potencia</p> \n",
    "  La derivada de $f(x) = x^n$ para $x \\in \\mathbb{R}$ y $n \\in \\mathbb{N}\\setminus \\{0\\}$ está dada por $f'(x) = nx^{n-1}$. De hecho, consideramos $f(x) = x^n$ para $x \\in \\mathbb{R}$ y $n \\in \\mathbb{N}\\setminus \\{0\\}$. Tenemos $$\\begin{equation}f(x + h) = (x + h)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} h^k\\end{equation}$$ Por lo tanto, $$\\begin{equation}\\begin{split}f(x + h) - f(x) & = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} h^k - x^n \\\\ & = \\sum_{k=1}^n \\binom{n}{k} x^{n-k} h^k.\\end{split}\\end{equation}$$ Dividiendo por $h$ y tomando el límite cuando $h \\to 0$, obtenemos $$\\begin{equation}\\begin{split}f'(x) & = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} \\\\ & = \\lim_{h \\to 0} \\sum_{k=1}^n \\binom{n}{k} x^{n-k} h^{k-1} \\\\ & = nx^{n-1}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "<div class=\"remark\"><p>Observación 1.1. Funciones en un subconjunto $U$ de $\\mathbb{R}$</p> \n",
    "  Para simplificar, consideramos funciones $f: \\mathbb{R} \\rightarrow \\mathbb{R}.$ Sin embargo, el concepto de derivada se puede extender a funciones definidas en un subconjunto $U$ de $\\mathbb{R}.$ Si una función $f: U \\rightarrow \\mathbb{R}$ está definida en un subconjunto $U$ de $\\mathbb{R}$, como es el caso de $f(x) = \\sqrt{x}$, definida en $U=\\mathbb{R}^+,$ la derivada de $f$ en un punto $x_0 \\in U$ está definida en un vecindario de $x_0$ contenido en $U,$ es decir, existen $r > 0$ tal que $f$ es diferenciable en $x_0 + \\epsilon \\in U$ para todo $|\\epsilon| < r.$ La función $f$ se dice entonces **diferenciable en todas partes** o diferenciable en resumen si es diferenciable en cada punto en el **interior** de $U$, el conjunto de puntos en $U$ tal que $\\{x + \\epsilon: |\\epsilon| < r\\} \\subset U$ para algún $r > 0$. Para puntos que yacen en el borde de $U$, el concepto de derivada es más sutil y requiere la definición de **derivadas unilaterales**, lo que significa que el límite en la definición de derivada se toma desde la izquierda o desde la derecha. Por ejemplo, la derivada de $f(x) = \\sqrt{x}$ en $x=0$ no está definida, ya que la función no está definida para valores negativos de $x$. Sin embargo, la derivada de $f(x) = \\sqrt{x}$ en $x=0$ está definida desde la derecha, y es igual a $1/2.$\n",
    "</div>\n",
    "\n",
    "### Reglas del cálculo\n",
    "Para un $x \\in \\mathbb{R}$ dado y dos funciones $f:\\mathbb{R}\\to\\mathbb{R}$ y $g:\\mathbb{R}\\to \\mathbb{R},$ la derivada\n",
    "de operaciones elementales en $f$ y $g$ como sus sumas, productos o composiciones se pueden derivar fácilmente de la\n",
    "definición de derivada, bajo condiciones apropiadas sobre las propiedades de diferenciabilidad de $f$ y $g$ en $x$. Por\n",
    "ejemplo, si las funciones $f$ y $g$ son diferenciables en $x,$ entonces la suma $af + bg$ y el producto $fg$ son\n",
    "diferenciables en $x$ para cualquier $a, b \\in \\mathbb{R}$, y sus derivadas están dadas por\n",
    "\n",
    "1. Linealidad: $(af + bg)'(x) = af'(x) + bg'(x).$\n",
    "2. Regla del producto: $(fg)'(x) = f'(x)g(x) + f(x)g'(x),$ donde $(fg)(x) + f(x)g(x).$\n",
    "\n",
    "La linealidad se puede verificar directamente a partir de la linealidad del operador de límite. Para la regla del\n",
    "producto, tenemos\n",
    "\n",
    "<div class=\"non-display-mobile\">\n",
    "$$\\begin{equation}\\begin{split} \\frac{(fg)(x + h) - (fg)(x)}{h} & = \\frac{f(x+h)g(x + h) - \\color{red}{f(x)g(x + h)}}{h} \\\\ & + \\frac{{\\color{red}f(x)g(x + h)} - fg(x)}{h} \\\\ & = g(x + h) \\frac{f(x + h) - f(x)}{h} \\\\ & + f(x)\n",
    "\\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"non-display-desktop\">\n",
    "$$\\begin{equation}\\begin{split} & \\frac{(fg)(x + h) - (fg)(x)}{h} = \\\\ & = \\frac{f(x+h)g(x + h) - \\color{red}{f(x)g(x + h)}}{h} \\\\ & + \\frac{{\\color{red}f(x)g(x + h)} - f(x)g(x)}{h} \\\\ & = g(x + h) \\frac{f(x + h) - f(x)}{h} \\\\ & + f(x)\n",
    "\\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "Si las derivadas de $g$ en $x$ y de $f$ en $g(x)$ existen, entonces la derivada de la composición $(f\\circ g)(x) =\n",
    "f(g(x))$ en $x$ está dada por la **regla de la cadena**:\n",
    "\n",
    "3. Regla de la cadena: $(f\\circ g)'(x) = f'(g(x))g'(x).$\n",
    "\n",
    "La regla de la cadena se puede derivar considerando el límite del cociente de la composición $(f\\circ g)(x)$ a medida\n",
    "que $h \\to 0$:\n",
    "\n",
    "<div class=\"non-display-mobile\">\n",
    "$$\\begin{equation}\\begin{split} \\frac{(f\\circ g)(x + h) - (f\\circ g)(x)}{h} & = \\frac{f(g(x + h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)} \\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"non-display-desktop\">\n",
    "$$\\begin{equation}\\begin{split} & \\frac{(f\\circ g)(x + h) - (f\\circ g)(x)}{h} = \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)} \\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "Como se puede ver, la linealidad y la regla del producto son consecuencias de la regla de la cadena, lo que hace que la\n",
    "regla de la cadena sea la piedra angular de la diferenciación.\n",
    "\n",
    "Por ahora, consideremos una función que se puede expresar como sumas, productos o composiciones de funciones elementales\n",
    "como $f(x) = \\exp(x) \\ln(x) + \\cos x^2.$ Su derivada se puede calcular aplicando las reglas mencionadas anteriormente a\n",
    "la descomposición de $f$ en operaciones y funciones elementales.\n",
    "\n",
    "<div class=\"example\"><p>Ejemplo 1.2. Aplicación de las reglas de diferenciación</p> \n",
    "  Consideremos la función $f(x) = \\exp(x) \\ln(x) + \\cos x^2.$ Tenemos $$f'(x) = \\exp(x) \\ln(x) + \\exp(x) \\frac{1}{x} - 2x \\sin x^2.$$ La derivada de $f$ en $x > 0$ se puede calcular paso a paso de la siguiente manera, denotando $\\operatorname*{sq}(x) := x^2,$\n",
    "  $$\\begin{equation}\\begin{split}f'(x) &  = (\\exp \\cdot \\ln)'(x) + (\\cos \\circ \\operatorname*{sq})'(x) \\\\ & = \\exp'(x) \\ln(x) + \\exp(x) \\ln'(x) + \\cos'(\\operatorname*{sq}(x)) \\operatorname*{sq}'(x) \\\\ & = \\exp(x) \\ln(x) + \\exp(x) \\frac{1}{x} - 2x \\sin x^2.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "### Notación de Leibniz\n",
    "La noción de derivada fue introducida por Isaac Newton y Gottfried Wilhelm Leibniz de forma independiente en el siglo\n",
    "XVIII. Este último consideró las derivadas como el cociente de variaciones infinitesimales. Es decir, denotando $y =\n",
    "f(x)$ una variable que depende de $x$ a través de $f$, Leibniz consideró la derivada de $f$ como el cociente:\n",
    "\n",
    "$$f' = \\frac{dy}{dx} \\quad \\text{con} \\quad f'(x) = \\left. \\frac{df}{dx}\\\\\\right|_{x}.$$\n",
    "\n",
    "Aquí, $dy$ y $dx$ son variaciones infinitesimales de $y$ y $x$, respectivamente, y el símbolo $\\left.\n",
    "\\frac{df}{dx}\\\\\\right|_{x}$ denota la derivada de $f$ en $x$. La notación $\\frac{dy}{dx}$ se llama **notación de\n",
    "Leibniz**. Es particularmente útil para expresar la regla de la cadena, ya que permite expresar la derivada de una\n",
    "composición de funciones como el producto de las derivadas de las funciones involucradas en la composición. Si tenemos\n",
    "para $z = g(y)$ y $y = f(x)$, entonces la regla de la cadena se puede expresar como:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}.$$\n",
    "\n",
    "Esto sugiere que las derivadas se multiplican al considerar composiciones de funciones. En la evaluación, la regla de la\n",
    "cadena en la notación de Leibniz recupera la fórmula presentada anteriormente como\n",
    "\n",
    "$$\\left. \\frac{dz}{dx}\\\\\\right|_{x} = \\left. \\frac{dz}{dy}\\\\\\right|_{y} \\left. \\frac{dy}{dx}\\\\\\right|_{x} =\n",
    "g'(f(x))f'(x) = (g\\circ f)'(x).$$\n",
    "\n",
    "## Funciones multivariables\n",
    "### Derivadas direccionales\n",
    "Consideremos ahora una función $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ de múltiples entradas $x = (x_1, \\ldots, x_n)\n",
    "\\in \\mathbb{R}^n$. El ejemplo más importante en el aprendizaje automático es una función que, a los parámetros\n",
    "$x\\in\\mathbb{R}^n$ de una red neuronal, asocia el valor de pérdida en $\\mathbb{R}$. Las variaciones de $f$ deben\n",
    "definirse a lo largo de direcciones específicas en $\\mathbb{R}^n$, como la variación $f(x + \\delta v) - f(x)$ de $f$\n",
    "alrededor de $x\\in\\mathbb{R}^n$ en la dirección $v\\in\\mathbb{R}^n$. Esta consideración conduce naturalmente a la\n",
    "definición de la derivada direccional de $f$ en $x$ en la dirección $v$.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.3. Derivada direccional</p>\n",
    "  La **derivada direccional** de una función $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ en un punto $x \\in \\mathbb{R}^n$ en la dirección $v \\in \\mathbb{R}^n$ se define como $$\\partial f(x)[v] = \\lim_{h \\to 0} \\frac{f(x + hv) - f(x)}{h}.$$ Si el límite existe, decimos que $f$ es diferenciable en $x$ en la dirección $v$.\n",
    "</div>\n",
    "\n",
    "Un ejemplo de derivada direccional consiste en calcular la derivada de una función $f$ en un punto $x$ en cualquiera de\n",
    "las direcciones canónicas $e_i$ de $\\mathbb{R}^n$, donde $e_i$ es el vector con un $1$ en la posición $i$ y $0$ en las\n",
    "demás.\n",
    "\n",
    "Esto nos permite definir la noción de **derivadas parciales**, denotadas para $i = 1, \\ldots, n$ por $$\\partial_i f(x)\n",
    ":= \\partial f(x)[e_i] = \\lim_{h \\to 0} \\frac{f(x + he_i) - f(x)}{h}.$$\n",
    "\n",
    "Esto también se denota en la notación de Leibniz por $\\partial_i f(x) = \\frac{\\partial f}{\\partial x_i}(x)$ o\n",
    "$\\partial_i f(x) = \\partial_{x_i} f(x).$ Al moverse solo a lo largo de la $i$-ésima coordenada de la función, la\n",
    "derivada parcial es similar a usar la función $\\phi(x_i)=f(x_1, \\cdots, x_n)$ alrededor de $x_i$, dejando todas las\n",
    "otras coordenadas fijas en sus valores $x_i$.\n",
    "\n",
    "### Gradientes\n",
    "Introducimos ahora el vector gradiente, que reúne las derivadas parciales. Primero recordamos las definiciones de mapa y\n",
    "forma lineal.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.4. Mapa lineal, forma lineal</p>\n",
    "  Un **mapa lineal** $A: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ es una función que satisface las siguientes propiedades para todo $x, y \\in \\mathbb{R}^n$ y $\\alpha, \\beta \\in \\mathbb{R}$: $$A(\\alpha x + \\beta y) = \\alpha A(x) + \\beta A(y).$$ Una **forma lineal** es un mapa lineal $A: \\mathbb{R}^n \\rightarrow \\mathbb{R}$.\n",
    "</div>\n",
    "\n",
    "La linealidad juega un papel crucial en la diferenciabilidad de una función.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.5. Diferenciabilidad, caso de una sola salida</p>\n",
    "  Una función $f:\\mathbb{R}^{n}\\to \\mathbb{R}$ es diferenciable en $x\\in \\mathbb{R}$ si su derivada direccional está definida en cualquier dirección $v\\in\\mathbb{R}^n$, es lineal en cualquier dirección, y si\n",
    "  $$\\lim_{||v||_{2}\\to 0}\\frac{|f(x + v) - f(x) - \\partial f(x)[v]|}{||v||_{2}} = 0$$\n",
    "</div>\n",
    "\n",
    "Ahora podemos introducir el gradiente.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.6. Gradiente</p>\n",
    "  El **gradiente** de una función diferenciable $f:\\mathbb{R}^n\\to \\mathbb{R}$ en un punto $x\\in\\mathbb{R}^n$ se define como el vector de derivadas parciales\n",
    "  $$\\nabla f(x) = \\begin{pmatrix} \\partial_1 f(x) \\\\ \\vdots \\\\ \\partial_n f(x) \\end{pmatrix} = \\begin{pmatrix} \\partial f(x)[e_1] \\\\ \\vdots \\\\ \\partial f(x)[e_n] \\end{pmatrix}.$$\n",
    "  Por linealidad, la derivada direccional de $f$ en $x$ en la dirección $v=\\sum_{i=1}^n v_i e_i$ está dada por\n",
    "  $$\\partial f(x)[v] = \\sum_{i=1}^n v_i\\partial_i f(x)[e_i] = \\langle v, \\nabla f(x) \\rangle.$$\n",
    "</div>\n",
    "\n",
    "En la definición anterior, el hecho de que el gradiente se pueda usar para calcular la derivada direccional es una mera\n",
    "consecuencia de la linealidad. Sin embargo, en casos más abstractos presentados en secciones posteriores, el gradiente\n",
    "se\n",
    "define a través de esta propiedad.\n",
    "\n",
    "Como ejemplo simple, cualquier función lineal de la forma $f(x)=a^\\top x = \\sum_{i=1}^{n}a_i x_i$ es diferenciable ya\n",
    "que tenemos $(a^\\top(x+v) - a^\\top x - a^\\top v)/||v||_2 = 0$ para cualquier $v$ y en particular para $||v||\\to 0.$\n",
    "Además, su gradiente se define naturalmente como $\\nabla f(x) = a.$\n",
    "\n",
    "Generalmente, para mostrar que una función es diferenciable y encontrar su gradiente, un enfoque es aproximar $f(x+v)$\n",
    "alrededor de $v=0$. Si podemos encontrar un vector $g$ tal que $$f(x+v) = f(x) + \\langle g, v\\rangle + o(||v||_2),$$\n",
    "entonces $f$ es diferenciable en $x$ ya que $\\langle g, \\cdot \\rangle$ es lineal. Además, $g$ es entonces el gradiente\n",
    "de $f$ en $x$.\n",
    "\n",
    "<div class=\"remark\"><p>Observación 1.2. Diferenciabilidad de Gateux y Fréchet</p> \n",
    "  Existen múltiples definiciones de diferenciabilidad. La presentada en la Definición 1.5 es sobre funciones **Fréchet\n",
    "  diferenciables**. Alternativamente, si $f:\\mathbb{R}^p \\to \\mathbb{R}$ tiene derivadas direccionales bien definidas\n",
    "  a lo largo de cualquier dirección, entonces la función es **Gateaux diferenciable**. Tenga en cuenta que la existencia\n",
    "  de derivadas direccionales en cualquier dirección no es una condición suficiente para que la función sea diferenciable.\n",
    "  En otras palabras, cualquier función Fréchet diferenciable es Gateaux diferenciable, pero el recíproco no es cierto. Como\n",
    "  contraejemplo, uno puede verificar que la función\n",
    "  $$f(x_1, x_2)=\\frac{x_1^3}{x_1^2 + x_2^2}$$ es Gateaux diferenciable en $0$ pero no Fréchet diferenciable en $0$\n",
    "  (porque la derivada direccional en 0 no es lineal).\n",
    "\n",
    "  Algunos autores también requieren que las funciones Gateux diferenciables tengan derivadas direccionales lineales a lo\n",
    "  largo de cualquier dirección. Estas aún no son funciones Fréchet diferenciables. De hecho, el límite en la Definición\n",
    "  1.5 es sobre cualquier vector que tienda a 0 (potencialmente de una manera patológica), mientras que las derivadas\n",
    "  direccionales consideran tales límites de manera única en términos de una sola dirección.\n",
    "\n",
    "  En el resto de este capítulo, todas las definiciones de diferenciabilidad son en términos de diferenciabilidad de\n",
    "  Fréchet.\n",
    "</div>\n",
    "\n",
    "El Ejemplo 1.3 ilustra cómo calcular el gradiente de la pérdida logística y validar su diferenciabilidad.\n",
    "\n",
    "<div class=\"example\"><p>Ejemplo 1.3. Gradiente de la pérdida logística</p> \n",
    "  Consideremos la pérdida logística\n",
    "  $$l(\\theta, y) := -y^\\top \\theta + \\log\\left(\\sum_{k=1}^m \\exp(\\theta_k)\\right),$$\n",
    "  que mide el error de predicción de los logits $\\theta\\in\\mathbb{R}^m$ para un objetivo $y\\in\\{e_1, \\ldots, e_m\\}.$\n",
    "  Calculemos el gradiente de esta pérdida. Queremos calcular el gradiente de $f(\\theta):=l(\\theta, y).$ Descompongamos\n",
    "  $f$ como $f = f + \\operatorname*{logsumexp}$ con $l(\\theta):=\\langle -y, \\theta\\rangle$ y $$\\operatorname*{logsumexp}(\\theta) := \\log\\left(\\sum_{k=1}^m \\exp(\\theta_k)\\right),$$ la función log-sum-exp. La función $l$ es lineal, por lo que es diferenciable con gradiente $\\nabla l(\\theta) = -y.$ Nos enfocamos entonces en $\\operatorname*{logsumexp}.$ Denotando $\\exp(\\theta) = (\\exp(\\theta_1), \\ldots, \\exp(\\theta_m)),$ tenemos\n",
    "  $$\\begin{equation}\\nabla \\operatorname*{logsumexp}(\\theta) = \\frac{\\exp(\\theta)}{\\sum_{k=1}^m \\exp(\\theta_k)} = \\operatorname*{softmax}(\\theta),\\end{equation}$$\n",
    "  donde $\\operatorname*{softmax}(\\theta)$ es la función softmax. El gradiente de la pérdida logística está dado por\n",
    "  $$\\nabla l(\\theta) = -y + \\operatorname*{softmax}(\\theta).$$\n",
    "</div>\n",
    "\n",
    "### Linealidad de los gradientes\n",
    "La noción de diferenciabilidad para funciones de múltiples entradas hereda naturalmente de la linealidad de las\n",
    "derivadas para funciones de una sola entrada. Para cualquier $u_1, \\dots, u_n\\in\\mathbb{R}$ y cualquier función de\n",
    "varias entradas $f_1, \\dots f_n$ que son diferenciables en $x\\in\\mathbb{R}^n$, la función $f(x) = \\sum_{i=1}^n u_i\n",
    "f_i(x)$ es diferenciable en $x$ con gradiente $\\nabla f(x) = \\sum_{i=1}^n u_i \\nabla f_i(x).$ Esta propiedad es una\n",
    "consecuencia directa de la linealidad del gradiente.\n",
    "\n",
    "<div class=\"theorem\"><p>Teorema 1.2. Linealidad de los gradientes</p>\n",
    "  Sea $f_1, \\ldots, f_n:\\mathbb{R}^n\\to\\mathbb{R}$ funciones diferenciables en $x\\in\\mathbb{R}^n$ y $u_1, \\ldots, u_n\\in\\mathbb{R}$. Entonces la función $f(x) = \\sum_{i=1}^n u_i f_i(x)$ es diferenciable en $x$ con gradiente $\\nabla f(x) = \\sum_{i=1}^n u_i \\nabla f_i(x).$\n",
    "</div>\n",
    "\n",
    "El gradiente define la dirección de ascenso más empinada de una función. Para ver por qué, observe que\n",
    "$$\\operatorname*{argmax}_{v\\in\\mathbb{R}^n, ||v||_2\\leq 1} \\partial f(x)[v] = \\operatorname*{argmax}_{v\\in\\mathbb{R}^n,\n",
    "||v||_2\\leq 1} \\langle v, \\nabla f(x)\\rangle = \\frac{\\nabla f(x)}{||\\nabla f(x)||_2},$$ donde asumimos que $\\nabla\n",
    "f(x)\\neq 0$ para evitar la división por cero. El gradiente $\\nabla f(x)$ es ortogonal al conjunto de nivel de la función\n",
    "$f$ en $x$, lo que significa que el gradiente apunta en la dirección del ascenso más empinado de $f$ en $x$. Por el\n",
    "contrario, el gradiente negativo $-\\nabla f(x)$ apunta en la dirección del descenso más empinado de $f$ en $x$. Esta\n",
    "observación motiva el desarrollo de algoritmos de optimización como el descenso de gradiente, que actualiza\n",
    "iterativamente los parámetros $x$, $x_{n+1} = x_n - \\alpha \\nabla f(x_n),$ donde $\\alpha > 0$ es un tamaño de paso. Por\n",
    "lo tanto, busca el mínimo de $f$ siguiendo la dirección de descenso más empinada.\n",
    "\n",
    "### Jacobianos\n",
    "Consideremos ahora una función $f:\\mathbb{R}^n\\to\\mathbb{R}^m$ que asigna un vector $x\\in\\mathbb{R}^n$ a un vector\n",
    "$f(x)\\in\\mathbb{R}^m$. Es decir, $f(x) = (f_1(x), \\ldots, f_m(x))$ con $f_i:\\mathbb{R}^n\\to\\mathbb{R}$ para $i=1,\n",
    "\\ldots, m,$ donde $f_i$ es la $i$-ésima componente de $f$. Un ejemplo típico en el aprendizaje automático es una red neuronal. La noción de derivada direccional puede ser extendida como una función vectorial compuesta por las derivadas direccionales de las componentes de $f$:\n",
    "\n",
    "$$\\partial f(x)[v] := \\lim_{\\delta \\to 0} \\frac{f(x + \\delta v) - f(x)}{\\delta} = \\lim_{\\delta \\to 0} \\begin{pmatrix}\n",
    "\\frac{f_1(x + \\delta v) - f_1(x)}{\\delta} \\\\ \\vdots \\\\ \\frac{f_1(x + \\delta v) - f_1(x)}{\\delta} \\end{pmatrix} \\in\n",
    "\\mathbb{R}^m,$$\n",
    "\n",
    "en donde los límites (asumiendo que existen) son aplicados a cada una de las coordenadas. La derivada direccional de $f$\n",
    "en la direccion $v\\in \\mathbb{R}^p$ es por lo tanto el vector que agrupa las derivadas de cada $f_j$, es decir,\n",
    "$\\partial f(x)[v] = (\\partial f_j (x)[v])_{j=1}^{m}$. En particular, podemos definir las **derivadas parciales** de $f$\n",
    "en $x$ como el vector: \n",
    "\n",
    "$$\\partial_i f(x) = \\partial f(x)[e_i] = \\begin{pmatrix} \\partial_i f_1(x) \\\\ \\vdots \\\\ \\partial_i f_m(x) \\end{pmatrix}\n",
    "\\in \\mathbb{R}^m.$$\n",
    "\n",
    "En cuanto a la definición habitual de la derivada, la derivada direccional de $f$ en $x$ puede proporcionar una\n",
    "aproximación lineal de una función en torno a una entrada de corriente en $x$ para una curva parametrizada\n",
    "$f:\\mathbb{R}\\to\\mathbb{R}^m$.\n",
    "\n",
    "Al igual que en caso de una sola salida, la diferenciabilidad se define no solo como la existencia de las derivadas\n",
    "direccionales, sino también por la linealidad en la dirección escogida.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.7. Diferenciabilidad, caso de múltiples salidas</p>\n",
    "  Una función $f:\\mathbb{R}^n\\to\\mathbb{R}^m$ es diferenciable en $x\\in\\mathbb{R}^n$ si su derivada direccional está definida en cualquier dirección $v\\in\\mathbb{R}^n$, es lineal en cualquier dirección, y si\n",
    "  $$\\lim_{||v||_{2}\\to 0}\\frac{||f(x + v) - f(x) - \\partial f(x)[v]||_2}{||v||_2} = 0.$$\n",
    "</div>\n",
    "\n",
    "La derivadas parciales de todas las componentes de $f$ en $x$ se pueden agrupar en una matriz, llamada **matriz\n",
    "Jacobiana**.\n",
    "\n",
    "<div class=\"definition\"><p>Definición 1.8. Matriz Jacobiana</p>\n",
    "  La **matriz Jacobiana** de una función $f:\\mathbb{R}^n\\to\\mathbb{R}^m$ en un punto $x\\in\\mathbb{R}^n$ es la matriz de derivadas parciales de $f$ en $x$, denotada por $\\pmb{\\partial}_f(x)$ o $\\pmb{\\nabla} f(x)$, y definida como\n",
    "  $$\\pmb{\\partial}_f(x) = \\begin{pmatrix} \\partial_1 f_1(x) & \\cdots & \\partial_n f_1(x) \\\\ \\vdots & \\ddots & \\vdots \\\\ \\partial_1\n",
    "  f_m(x) & \\cdots & \\partial_n f_m(x) \\end{pmatrix} \\in M_{m\\times n}(\\mathbb{R}).$$\n",
    "  El Jacobiano se puede representar apilando columnas de derivadas parciales o filas de gradientes de las componentes de $f,$\n",
    "  $$\\pmb{\\partial}_f(x) = \\big(\\partial_1 f(x), \\dots, \\partial_n f(x)\\big) = \\begin{pmatrix} \\nabla f_1(x)^\\top \\\\ \\vdots \\\\ \\nabla f_m(x)^\\top \\end{pmatrix}.$$\n",
    "  En virtud de la linealidad de los gradientes, la derivada direccional de $f$ en $x$ en la dirección $v = \\sum_{i=1}^n v_i e_i$ está dada por\n",
    "  $$\\partial f(x)[v] = \\sum_{i=1}^n v_i \\partial_i f(x) = \\pmb{\\partial}_f(x)v.$$\n",
    "</div>\n",
    "\n",
    "Notemos que hemos usado la notación $\\pmb{\\partial}$ en negrita para denotar la matriz Jacobiana, en contraste con la\n",
    "notación $\\partial$ para las derivadas parciales. La matriz Jacobiana generaliza los conceptos de derivadas y gradientes\n",
    "presentados anteriormente. En particular, para el caso de una sola variable, para mostrar que una función es\n",
    "diferenciable, un enfoque es aproximar $f(x+v)$ alrededor de punto $v=0$. Si podemos encontrar una transformación lineal\n",
    "$l$ tal que $$f(x+v) = f(x) + l(v) + o(||v||_2),$$ entonces $f$ es diferenciable en $x$ y $l$ es el Jacobiano de $f$, es\n",
    "decir, $l[v] = \\pmb{J}v$ en donde $\\pmb{J} = \\pmb{\\partial} f(x).$\n",
    "\n",
    "Un ejemplos simple, es la función $f(x) = Ax$ con $A\\in M_{m\\times n}(\\mathbb{R})$ y $x\\in\\mathbb{R}^n$. $f$ es\n",
    "diferenciable, dado que todas las componentes de $f$ son lineales. El Jacobiano de $f$ es $\\pmb{\\partial} f(x) = A.$\n",
    "\n",
    "<div class=\"remark\"><p>Observación 1.3. Casos especiales de los Jacobianos</p> \n",
    "  Para funciiones $f:\\mathbb{R}^n\\to\\mathbb{R}$, el Jacobiano es un vector fila identificado con el transpuesto del gradiente, es decir, $$\\pmb{\\partial} f(x) = \\nabla f(x)^\\top.$$ Para funciones $f:\\mathbb{R}\\to\\mathbb{R}^m$, el Jacobiano es un vector columna de las derivadas direccionales, denotado por\n",
    "  $$\\pmb{\\partial} f(x) = f'(x) = \\begin{pmatrix} \\partial f_1(x) \\\\ \\vdots \\\\ \\partial f_m(x) \\end{pmatrix} \\in M_{m\\times 1}(\\mathbb{R}).$$\n",
    "\n",
    "  Para una función $f:\\mathbb{R}\\to\\mathbb{R}$, el Jacobiano se reduce a la derivada de la función, es decir,\n",
    "  $\\pmb{\\partial} f(x) = f'(x)\\in \\mathbb{R}.$\n",
    "</div>\n",
    "\n",
    "En el siguiente ejemplo, se ilustra la forma de la matriz Jacobiana para la aplicación elemento por elemento de una\n",
    "función diferenciable, como la función softplus. Este ejemplo muestra que el Jacobiano toma la forma de una matriz\n",
    "diagonal. Como consencuencia, la derivada direccional asociada con esta función se da simplemente por un producto\n",
    "elemento por elemento en lugar de un producto matricial-vector, como se sugiere en la definición 1.8.\n",
    "\n",
    "<div class=\"example\"><p>Ejemplo 1.4. Jacobiano de la función softplus</p> \n",
    "  Consideremos la función softplus $f:\\mathbb{R}^n\\to\\mathbb{R}^n$ definida por \n",
    "  $$f(x) = \\begin{pmatrix} \\sigma(x_1) \\\\ \\vdots \\\\ \\sigma(x_n) \\end{pmatrix},$$ \n",
    "  donde $\\sigma(x) = \\log(1 + \\exp(x))$ es la función softplus. La función softplus es diferenciable con derivada $\\sigma'(x) = \\frac{\\exp(x)}{1 + \\exp(x)} = \\frac{1}{1 + \\exp(-x)} = \\sigma(x)(1 - \\sigma(x)).$ El Jacobiano de $f$ es entonces \n",
    "  $$\\pmb{\\partial} f(x) = \\begin{pmatrix} \\sigma'(x_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & \\sigma'(x_n) \\end{pmatrix} = \\operatorname*{diag}(\\sigma'(x_1), \\ldots, \\sigma'(x_n)).$$\n",
    "</div>\n",
    "\n",
    "#### Variaciones de la matriz Jacobiana\n",
    "\n",
    "En lugar de considerar las variaciones de $f$ a lo largo de una dirección de entrada $v\\in\\mathbb{R}^n,$ podemos\n",
    "considerar las variaciones de $f$ a lo largo de una dirección de salida $w\\in\\mathbb{R}^m,$ es decir, calcular los\n",
    "gradientes $\\nabla (u^\\top f)(x)$ para $u\\in\\mathbb{R}^m.$ En donde $(u^\\top f)(x)$ se define como el producto escalar de $u$ y $f(x),$ es decir, $$(u^\\top f)(x) = \\sum_{i=1}^m u_i f_i(x).$$ En particular, podemos considerar calcular los gradientes $\\nabla f_i(x)$ para $i=1, \\ldots, m.$ de cada función coordenada $f_i = e_i^\\top f,$ en $x\\in\\mathbb{R}^n.$, donde $e_i$ es el vector canónico con un $1$ en la posición $i$ y $0$ en las demás. Las variaciones infinitesimales de $f$ en $x$ en la dirección $u = \\sum_{i=1}^m u_i e_i$ se pueden expresar en términos de los gradientes de las funciones por\n",
    "$$\\nabla (u^\\top f)(x) = \\sum_{i=1}^m u_i \\nabla f_i(x) = \\pmb{\\partial} f(x)^\\top u\\in\\mathbb{R}^n.$$\n",
    "Donde $\\pmb{\\partial}f(x)^\\top$ es el transpuesto de la matriz Jacobiana de $f$ en $x.$ Usando la definición de la\n",
    "derivada como un límite, obtenemos para $i=1, \\ldots, m,$\n",
    "$$\\nabla_i(u^\\top f)(x) = \\[\\pmb{\\partial}f(x)^\\top u\\]_i = \\lim_{\\delta\\to 0} \\frac{u^\\top (f(x + \\delta e_i) -\n",
    "f(x))}{\\delta}.$$\n",
    "\n",
    "### Regla de la cadena\n",
    "La regla de la cadena para funciones de múltiples variables es una generalización de la regla de la cadena para\n",
    "funciones del tipo $f:\\mathbb{R}\\to\\mathbb{R}.$\n",
    "\n",
    "<div class=\"theorem\"><p>Teorema 1.3. Regla de la cadena</p>\n",
    "  Sea $f:\\mathbb{R}^n\\to\\mathbb{R}^m$ y $g:\\mathbb{R}^m\\to\\mathbb{R}^p$ funciones diferenciables en $x\\in\\mathbb{R}^n$ y $f(x)\\in\\mathbb{R}^m.$ Entonces la composición $h = g\\circ f$ es diferenciable en $x$ y su Jacobiano está dado por\n",
    "  $$\\pmb{\\partial} h(x) = \\pmb{\\partial} g(f(x))\\pmb{\\partial} f(x).$$\n",
    "</div>\n",
    "\n",
    "**Demostración.** La regla de la cadena se puede demostrar considerando la derivada direccional de $h$ en $x$ en la\n",
    "dirección $v\\in\\mathbb{R}^n.$ La derivada direccional de $h$ en $x$ en la dirección $v$ se puede expresar como\n",
    "$$h(x + v) - h(x) = g(f(x) + \\pmb{\\partial} f(x)v + o(||v||_2)) = g(f(x)) + \\pmb{\\partial} g(f(x))\\pmb{\\partial} f(x)v +\n",
    "o(||v||_2),$$ \n",
    "donde hemos usado la diferenciabilidad de $f$ y $g$ en $x$ y la definición de la derivada direccional. Por lo tanto,\n",
    "$h$ es diferenciable en $x$ y su Jacobiano está dado por $\\pmb{\\partial} h(x) = \\pmb{\\partial} g(f(x))\\pmb{\\partial}\n",
    "f(x).$\n",
    "\n",
    "El teorema 1.3 puede ser visto como la piedra angular de cualquier cálculo de derivadas. Por\n",
    "ejemplo, puede usarse para demostrar la linealidad o la regla del producto asociadas a las derivadas de las funciones\n",
    "del tipo $f:\\mathbb{R}\\to\\mathbb{R}.$\n",
    "\n",
    "Cuando $g:\\mathbb{R}^m\\to\\mathbb{R}$ y $f:\\mathbb{R}^n\\to\\mathbb{R}^m,$ con el uso de observación 1.3, obtenemos una\n",
    "expresión más simple para $\\nabla (g\\circ f)(x)$ en términos de los gradientes de $f$ y $g$ en $x.$\n",
    "\n",
    "<div class=\"theorem\"><p>Teorema 1.4. Regla de cadena para el caso vector-escalar</p>\n",
    "  Sea $f:\\mathbb{R}^n\\to\\mathbb{R}^m$ y $g:\\mathbb{R}^m\\to\\mathbb{R}$ funciones diferenciables en $x\\in\\mathbb{R}^n$ y $f(x)\\in\\mathbb{R}^m.$ Entonces la composición $h = g\\circ f$ es diferenciable en $x$ y su gradiente está dado por\n",
    "  $$\\nabla h(x) = \\nabla (g\\circ f)(x) = \\nabla f(x)^\\top\\nabla g(f(x)).$$\n",
    "</div>\n",
    "\n",
    "Esto se puede ilustrar con el siguiente ejemplo.\n",
    "\n",
    "<div class=\"example\"><p>Regresión Lineal</p>\n",
    "  Consideremos los cuadrados residuales de una regresión lineal de $n$ puntos de datos $(x_i, y_i)\\in\\mathbb{R}^d \\times \\mathbb{R}$ para $i=1, \\ldots, n.$ La regresión lineal busca minimizar la función de pérdida $f:\\mathbb{R}^2\\to\\mathbb{R}$ dada por $$f(v) = ||Xv - y||_2^2 = \\sum_{i=1}^n (x_i^\\top v - y_i)^2,$$ donde $X = (x_1, \\ldots, x_n)^\\top \\in M_{n $ es la matriz de datos y $y = (y_1, \\ldots, y_n)^\\top$ es el vector de etiquetas. La función de pérdida $f$ es diferenciable y su gradiente está dado por $$\\nabla f(v) = 2X^\\top(Xv - y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
