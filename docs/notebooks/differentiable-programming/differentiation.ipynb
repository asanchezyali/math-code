{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation\n",
    "**Author:** Alejandro Sánchez Yalí\n",
    "\n",
    "In this chapter, we review key differentiation concepts. In particular, we emphasize on the fundamental role played by\n",
    "linear approximations in the context of numerical differentiation. We also discuss the concept of automatic\n",
    "differentiation, which is a powerful tool for computing derivatives of functions implemented in computer programs.\n",
    "\n",
    "## Univariate differentiation\n",
    "### Derivatives\n",
    "Before studying derivatives, we recall the definition of function continuity.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.1. Continuous function</p> \n",
    "  A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is **continuous at a point** $x_0$ if $$\\lim_{x \\to x_0} f(x) = f(x_0).$$\n",
    "  A function $f$ is **continuous** if it is continuous at every point in its domain.\n",
    "</div>\n",
    "\n",
    "In the following, we use Landau's notation to describe the behavior of functions near a point. We write $$f(x) = o\\big(g(x)\\big) \\quad \\text{as} \\quad x \\to x_0$$\n",
    "if $$\\lim_{x \\to x_0} \\frac{|f(x)|}{|g(x)|} = 0.$$\n",
    "\n",
    "That is, $f(x)$ is much smaller than $g(x)$ as $x$ approaches $x_0.$ For example, $f$ is continuous at $x_0$ \n",
    "if $$f(x_0 + \\delta) = f(x_0) + o(1) \\quad \\text{as} \\quad \\delta \\to 0.$$\n",
    "\n",
    "We now introduce the concept of derivative. Consider a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ and a point\n",
    "$x_0$ in its domain. Its value on an interval $[x_0, x_0 + h]$ can be approximated by the secant between $\\big(x_0, f(x_0)\\big)$\n",
    "and $\\big(x_0 + h, f(x_0 + h)\\big)$. The slope of this **secant** is given by the difference quotient $$\\frac{f(x_0 + h) -\n",
    "f(x_0)}{h}.$$ In the limit of an infinitesimal $h$, the secant converges to the **tangent** at $\\big(x_0, f(x_0)\\big)$. The slope\n",
    "of this tangent is the derivative of $f$ at $x_0$, denoted by $f'(x_0).$ The definition below\n",
    "formalizes this intuition.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.2. Derivative</p>\n",
    "  The **derivative** of a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ at a point $x_0$ is defined as \n",
    "  $$f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}.$$ If $f'(x_0)$ is well-defined at a particular $x_0$, \n",
    "  we say that the function $f$ is differentiable at $x_0$.\n",
    "</div>\n",
    "\n",
    "Here, and in the following definitions, if $f$ is differentiable at any $x$, we say that it is **differentiable\n",
    "everywhere** of simply **differentiable**. If $f$ is differentiable at a given $x$, then it is necessarily continuous at\n",
    "$x$.\n",
    "\n",
    "<div class=\"theorem\"><p>Theorem 1.1. Differentiability implies continuity</p> \n",
    "  If a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is differentiable at a point $x_0$, then it is continuous at $x_0$.\n",
    "</div>\n",
    "\n",
    "*Proof.* The proof follows from the definition of derivative. We have $$f(x_0 + h) = f(x_0) + f'(x_0)h + o(h) \\quad\n",
    "\\text{as} \\quad h \\to 0.$$ Since $f'(x_0)h + o(h) = o(1)$ as $h \\to 0$, we have that  $$\\lim_{h \\to 0} |f(x_0 + h) -\n",
    "f(x_0)| = 0.$$ Therefore, $f$ is continuous at $x_0$.\n",
    "\n",
    "In addition to enabling the computation of the slope of a function at a point, the derivative provides information about\n",
    "the **mononicity** of $f$ near that point. For example, if $f'(x_0) > 0$, then $f$ is increasing near $x_0$. If $f'(x_0) < 0$,\n",
    "then $f$ is decreasing near $x_0$. If $f'(x_0) = 0$, then $f$ has a local extremum at $x_0$. Such information can be\n",
    "used to develop iterative algorithms to minimize or maximize $f$ by computing iterates of the form $$x_{n+1} = x_n - \\alpha f'(x_n),$$\n",
    "where $\\alpha$ is a step size. If $\\alpha > 0$, the algorithm converges to a local minimum of $f$. If $\\alpha < 0,$ it\n",
    "converges to a local maximum. If $f$ is convex, the algorithm converges to the global minimum. \n",
    "\n",
    "For several elementary functions, the derivative can be computed analytically. \n",
    "\n",
    "<div class=\"example\"><p>Example 1.1. Derivative of power function</p> \n",
    "  The derivative of $f(x) = x^n$ for $x \\in \\mathbb{R}$ and $n \\in \\mathbb{N}\\setminus \\{0\\}$ is given by $f'(x) = nx^{n-1}$. In fact, we consider $f(x) = x^n$ for $x \\in \\mathbb{R}$ and $n \\in \\mathbb{N}\\setminus \\{0\\}$. We have $$\\begin{equation}f(x + h) = (x + h)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} h^k\\end{equation}$$ Therefore, $$\\begin{equation}\\begin{split}f(x + h) - f(x) & = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} h^k - x^n \\\\ & = \\sum_{k=1}^n \\binom{n}{k} x^{n-k} h^k.\\end{split}\\end{equation}$$ Dividing by $h$ and taking the limit as $h \\to 0$, we obtain $$\\begin{equation}\\begin{split}f'(x) & = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} \\\\ & = \\lim_{h \\to 0} \\sum_{k=1}^n \\binom{n}{k} x^{n-k} h^{k-1} \\\\ & = nx^{n-1}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "<div class=\"remark\"><p>Remark 1.1. Functions on a subset $U$ of $\\mathbb{R}$</p> \n",
    "  For simplicity, we consider functions $f: \\mathbb{R} \\rightarrow \\mathbb{R}.$ However, the concept of derivative can be extended to functions defined on a subset $U$ of $\\mathbb{R}.$ If a fuction $f: U \\rightarrow \\mathbb{R}$ is defined on a subset $U$ of $\\mathbb{R}$, as it is the case for $f(x) = \\sqrt{x}$, defined on $U=\\mathbb{R}^+,$ the derivative of $f$ at a point $x_0 \\in U$ is defined on a neighborhood of $x_0$ contained in $U$, that is, there exist $r > 0$ such that $f$ is differentiable on $x_0 + \\epsilon \\in U$ for all $|\\epsilon| < r.$ The function $f$ is then said **differentiable everywhere** or differentiable for short if it is differentiable at every point in the **interior** of $U$, the set of points in $U$ such that $\\{x + \\epsilon: |\\epsilon| < r\\} \\subset U$ for some $r > 0$. For points lying at the boundary of $U$, the concept of derivative is more subtle and requires the definition of **one-sided derivatives**, meaning that the limit in the definition of derivative is taken from the left or from the right. For example, the derivative of $f(x) = \\sqrt{x}$ at $x=0$ is not defined, since the function is not defined for negative values of $x$. However, the derivative of $f(x) = \\sqrt{x}$ at $x=0$ is defined from the right, and it is equal to $1/2.$\n",
    "</div>\n",
    "\n",
    "### Calculus rules\n",
    "\n",
    "For a given $x \\in \\mathbb{R}$ and two functions $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to \\mathbb{R},$ the\n",
    "derivative of elementary operations on $f$ and $g$ such as their sums, products or compositions can easily be derived\n",
    "from the definition of the derivative, under appropriate conditions on the differentiability properties of $f$ and $g$\n",
    "at $x$. For example, if the functions $f$ and $g$ are differentiable at $x,$ then the sum $af + bg$ and the product $fg$\n",
    "are differentiable at $x$ for any $a, b \\in \\mathbb{R}$, and their derivatives are given by \n",
    "\n",
    "1. Linearity: $(af + bg)'(x) = af'(x) + bg'(x).$\n",
    " \n",
    "2. Product rule: $(fg)'(x) = f'(x)g(x) + f(x)g'(x),$ where $(fg)(x) + f(x)g(x).$\n",
    "\n",
    "The linearity can be verified directly from the linearity of the limit operator. For the product rule, we have\n",
    "\n",
    "<div class=\"non-display-mobile\">\n",
    "$$\\begin{equation}\\begin{split} \\frac{(fg)(x + h) - (fg)(x)}{h} & = \\frac{f(x+h)g(x + h) - \\color{red}{f(x)g(x + h)}}{h} \\\\ & + \\frac{{\\color{red}f(x)g(x + h)} - fg(x)}{h} \\\\ & = g(x + h) \\frac{f(x + h) - f(x)}{h} \\\\ & + f(x)\n",
    "\\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"non-display-desktop\">\n",
    "$$\\begin{equation}\\begin{split} & \\frac{(fg)(x + h) - (fg)(x)}{h} = \\\\ & = \\frac{f(x+h)g(x + h) - \\color{red}{f(x)g(x + h)}}{h} \\\\ & + \\frac{{\\color{red}f(x)g(x + h)} - f(x)g(x)}{h} \\\\ & = g(x + h) \\frac{f(x + h) - f(x)}{h} \\\\ & + f(x)\n",
    "\\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "If the derivatives of $g$ at $x$ and of $f$ at $g(x)$ exist, then the derivative of the composition $(f\\circ g)(x) =\n",
    "f(g(x))$ at $x$ is given by the **chain rule**:\n",
    "\n",
    "3. Chain rule: $(f\\circ g)'(x) = f'(g(x))g'(x).$\n",
    "\n",
    "The chain rule can be derived by considering the limit of the difference quotient of the composition $(f\\circ g)(x)$ as\n",
    "$h \\to 0$:\n",
    "\n",
    "<div class=\"non-display-mobile\">\n",
    "$$\\begin{equation}\\begin{split} \\frac{(f\\circ g)(x + h) - (f\\circ g)(x)}{h} & = \\frac{f(g(x + h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)} \\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"non-display-desktop\">\n",
    "$$\\begin{equation}\\begin{split} & \\frac{(f\\circ g)(x + h) - (f\\circ g)(x)}{h} = \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x + h)) - f(g(x))}{g(x + h) - g(x)} \\frac{g(x + h) - g(x)}{h}.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "As seen in the sequel, the linearity and the product rule can be seen as byproducts of the chain rule, making the chain\n",
    "rule the cornerstone of differentiation.\n",
    "\n",
    "For now, consider a function that can be expressed as sums, products aor compositions of elementary functions such as\n",
    "$f(x) = \\exp(x) \\ln(x) + \\cos x^2.$ Its derivative can be computed by applying the aforementioned rules on the\n",
    "decomposition of $f$ into elementary operations and functions. \n",
    "\n",
    "\n",
    "<div class=\"example\"><p>Example 1.2. Applying rules of differentiation</p> \n",
    "Consider the function $f(x) = \\exp(x) \\ln(x) + \\cos x^2.$ We have $$f'(x) = \\exp(x) \\ln(x) + \\exp(x) \\frac{1}{x} - 2x \\sin x^2.$$ The derivative of $f$ on $x > 0$ can be computed step by step as follows, denoting $\\operatorname*{sq}(x) := x^2$,\n",
    "$$\\begin{equation}\\begin{split}f'(x) &  = (\\exp \\cdot \\ln)'(x) + (\\cos \\circ \\operatorname*{sq})'(x) \\\\ & = \\exp'(x) \\ln(x) + \\exp(x) \\ln'(x) + \\cos'(\\operatorname*{sq}(x)) \\operatorname*{sq}'(x) \\\\ & = \\exp(x) \\ln(x) + \\exp(x) \\frac{1}{x} - 2x \\sin x^2.\\end{split}\\end{equation}$$\n",
    "</div>\n",
    "\n",
    "### Leibniz's notation\n",
    "\n",
    "The notion of derivative was first introduced independently by Isaac Newton and Gottfried Wilhelm Leibniz in the 18th\n",
    "century. The latter considered derivatives as the quotient of infinitesimals variations. Namely, denoting $y = f(x)$ a\n",
    "variable depending on $x$ through $f$, Leibniz considered the derivative of $f$ as the quotient:\n",
    "\n",
    "$$f' = \\frac{dy}{dx} \\quad \\text{with} \\quad f'(x) = \\left. \\frac{df}{dx}\\\\\\right|_{x}.$$\n",
    "\n",
    "where $dy$ and $dx$ are infinitesimal variations of $y$ and $x$, respectively and the symbol $\\left.\n",
    "\\frac{df}{dx}\\\\\\right|_{x}$ denotes the derivative of $f$ at $x$. The notation $\\frac{dy}{dx}$ is called **Leibniz's\n",
    "notation**. It is particularly useful to express the chain rule, as it allows to express the derivative of a composition\n",
    "of functions as the product of the derivatives of the functions involved in the composition. If we have for $z = g(y)$\n",
    "and $y = f(x)$, then the chain rule can be expressed as:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}.$$\n",
    "\n",
    "This hints that derivatives are multiplied when considering compositions of functions. At evaluation, the chain rule in\n",
    "Leibniz notation recovers the formula presented above as\n",
    "\n",
    "$$\\left. \\frac{dz}{dx}\\\\\\right|_{x} = \\left. \\frac{dz}{dy}\\\\\\right|_{y} \\left. \\frac{dy}{dx}\\\\\\right|_{x} =\n",
    "g'(f(x))f'(x) = (g\\circ f)'(x).$$\n",
    "\n",
    "\n",
    "## Multivariate functions\n",
    "### Directional derivatives\n",
    "Let us now consider a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ of multiple inputs $x = (x_1, \\ldots, x_n) \\in\n",
    "\\mathbb{R}^n$. The most important example in machibne learning is a functions which, to the parameters\n",
    "$x\\in\\mathbb{R}^n$ of a neural network, associates the loss value in $\\mathbb{R}$. Variations of $f$ need to be defined\n",
    "along specific directions in $\\mathbb{R}^n$, such as the variation $f(x + \\delta v) - f(x)$ of $f$ around\n",
    "$x\\in\\mathbb{R}^n$ in the direction $v\\in\\mathbb{R}^n$. This consideration naturally leads to the definition of the\n",
    "directional derivative of $f$ at $x$ in the direction $v$.\n",
    "\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.3. Directional derivative</p>\n",
    "  The **directional derivative** of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ at a point $x \\in \\mathbb{R}^n$ in the direction $v \\in \\mathbb{R}^n$ is defined as $$\\partial f(x)[v] = \\lim_{h \\to 0} \\frac{f(x + hv) - f(x)}{h}.$$ If the limit exists, we say that $f$ is differentiable at $x$ in the direction $v$.\n",
    "</div>\n",
    "\n",
    "One example of directional derivative consists in computing the derivative of a function $f$ at a point $x$ in any of\n",
    "the canonical directions $e_i$ of $\\mathbb{R}^n$, where $e_i$ is the vector with a $1$ at the $i$-th position and $0$.\n",
    "\n",
    "This allows us to define the notion of **partial derivatives**, denoted for $i = 1, \\ldots, n$ by $$\\partial_i f(x)\n",
    ":=\\partial f(x)[e_i] = \\lim_{h \\to 0} \\frac{f(x + he_i) - f(x)}{h}.$$ \n",
    "\n",
    "This also denoted in Leibniz's notation by $\\partial_i f(x) = \\frac{\\partial f}{\\partial x_i}(x)$  or $\\partial_i f(x)\n",
    "= \\partial_{x_i} f(x).$ By moving along only the $i$-th coordinate of the function, the partial derivative is akin to\n",
    "using the function $\\phi(x_i)=f(x_1, \\cdots, x_n)$ around $x_i$, letting all other coordinates fixed at their values\n",
    "$x_i$.\n",
    "\n",
    "### Gradients\n",
    "We now introduce the gradient vector, which gathers the partial derivatives. We first recall the definitions of linear\n",
    "map and linear form.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.4. Linear map, linear form</p>\n",
    "  A **linear map** $A: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is a function that satisfies the following properties for all $x, y \\in \\mathbb{R}^n$ and $\\alpha, \\beta \\in \\mathbb{R}$: $$A(\\alpha x + \\beta y) = \\alpha A(x) + \\beta A(y).$$ A **linear form** is a linear map $A: \\mathbb{R}^n \\rightarrow \\mathbb{R}$.\n",
    "</div>\n",
    "\n",
    "Linearity plays a crucial role in the differentiability of a function.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.5. Differentiability, single-output case</p>\n",
    "  A function $f:\\mathbb{R}^{n}\\to \\mathbb{R}$ is differentiable at $x\\in \\mathbb{R}$ if its directional derivative is defined along any direction $v\\in\\mathbb{R}^n$, linear in any direction, and if\n",
    "  $$\\lim_{||v||_{2}\\to 0}\\frac{|f(x + v) - f(x) - \\partial f(x)[v]|}{||v||_{2}} = 0$$\n",
    "</div>\n",
    "\n",
    "We can now introduce the gradient.\n",
    "\n",
    "<div class=\"definition\"><p>Definition 1.6. Gradient</p>\n",
    "  The **gradient** of a differentiable function $f:\\mathbb{R}^n\\to \\mathbb{R}$ at a point $x\\in\\mathbb{R}^n$ is defined as the vector of partial derivatives\n",
    "  $$\\nabla f(x) = \\begin{pmatrix} \\partial_1 f(x) \\\\ \\vdots \\\\ \\partial_n f(x) \\end{pmatrix} = \\begin{pmatrix} \\partial f(x)[e_1] \\\\ \\vdots \\\\ \\partial f(x)[e_n] \\end{pmatrix}.$$\n",
    "  By linearity, the directional derivative of $f$ at $x$ in the direction $v=\\sum_{i=1}^n v_i e_i$ is given by\n",
    "  $$\\partial f(x)[v] = \\sum_{i=1}^n v_i\\partial_i f(x)[e_i] = \\langle v, \\nabla f(x) \\rangle.$$ \n",
    "</div>\n",
    "\n",
    "In the definition above, the fact that the gradient can be used to compute the directional derivative is a mere\n",
    "consequence of the linearity. However, in more abstract cased presented in later sections, the gradient is defined\n",
    "through this property. \n",
    "\n",
    "As a simple example, any linear function of the form $f(x)=a^\\top x = \\sum_{i=1}^{n}a_i x_i$ is differentiable as we\n",
    "have $(a^\\top(x+v) - a^\\top x - a^\\top v)/||v||_2 = 0$ for any $v$ adn in particular for $||v||\\to 0$. Moreover, its\n",
    "gradient is naturally given by $\\nabla f(x) = a.$\n",
    "\n",
    "Generally, to show that a function is differentiable and find its gradient, one approach is to approximate $f(x + v)$\n",
    "around $v = 0$. If we can find a vector $g$ such that\n",
    "$$f(x + v) = f(x) + \\langle g, v\\rangle + o(||v||_2),$$\n",
    "then $f$ is differentiable at $x$ since $\\langle g, \\cdot \\rangle$ is linear. Moreover, $g$ is then the gradient of $f$\n",
    "at $x$.\n",
    "\n",
    "<div class=\"remark\"><p>Remark 1.2. Gateux and Fréchet differentiability</p> \n",
    "  Multiple definitions of differentiability exist. The one presented in Definition 1.5 is about **Fréchet differentiable** functions. Alternatively, if $f:\\mathbb{R}^p \\to \\mathbb{R}$ has well-defined directional derivatives along any directions then the function is **Gateaux differentiable**. Note that the existence of directional derivatives in any directions is not a sufficient condition for the function to be differentiable. In other words, any Fréchet differentiable function is Gateaux  differentiable, but the converse is not true. As a counter-example, one can verify that function\n",
    "  $$f(x_1, x_2)=\\frac{x_1^3}{x_1^2 + x_2^2}$$ is Gateaux differentiable at $0$ but not Fréchet differentiable at $0$ (because the directional derivative at 0 is no linear).\n",
    "\n",
    "  Somo author also require Gateux differentiable functions to have linear directional derivatives along any direction.\n",
    "  These are still not Fréchet differentiable functions. Indeed, the limit in Definition 1.5 is over any vectors tending\n",
    "  to 0 (potentially in a pathological way), while directional derivatives look at such limits uniquely in terms of a\n",
    "  single direction.\n",
    "\n",
    "  In the remainder of this chapter, all definitions of differentiability are in terms of Fréchet differentiability.\n",
    "</div>\n",
    "\n",
    "Example 1.3 illustrates how to compute the gradient of the logistic loss and validate its differentiability.\n",
    "\n",
    "<div class=\"example\"><p>Example 1.3. Gradient of logistic loss</p> \n",
    "Consider the logistic loss\n",
    "$$l(\\theta, y) := -y^\\top \\theta + \\log\\left(\\sum_{k=1}^m \\exp(\\theta_k)\\right),$$\n",
    "that measures the prediction error of the logits $\\theta\\in\\mathbb{R}^m$ for a target $y\\in\\{e_1, \\ldots, e_m}.$ Let us compute the gradient of this loss. We want to compute the gradient of $f(\\theta):=l(\\theta, y)$. Let us decompose $f$ as $f = f + \\operatorname*{logsumexp}$ with $l(\\theta):=\\langle -y, \\theta\\rangle$ and $$\\operatorname*{logsumexp}(\\theta) := \\log\\left(\\sum_{k=1}^m \\exp(\\theta_k)\\right),$$ the log-sum-exp function. The function $l$ is linear so differentiable with gradient $\\nabla l(\\theta) = -y.$ We therefore focus $\\operatorname*{logsumexp}.$ Denoting $\\exp(\\theta) = (\\exp(\\theta_1), \\ldots, \\exp(\\theta_m)),$ we have\n",
    "$$\\begin{equation}\\begin{split}\\nabla \\operatorname*{logsumexp}(\\theta) & = \\begin{pmatrix} \\partial_1 \\operatorname*{logsumexp}(\\theta) \\\\ \\vdots \\\\ \\partial_m \\operatorname*{logsumexp}(\\theta) \\end{pmatrix} \\\\ & = \\begin{pmatrix} \\frac{\\exp(\\theta_1)}{\\sum_{k=1}^m \\exp(\\theta_k)} \\\\ \\vdots \\\\ \\frac{\\exp(\\theta_m)}{\\sum_{k=1}^m \\exp(\\theta_k)} \\end{pmatrix} \\\\ & = \\frac{\\exp(\\theta)}{\\sum_{k=1}^m \\exp(\\theta_k)} = \\operatorname*{softmax}(\\theta),\\end{split}\\end{equation}$$\n",
    "where $\\operatorname*{softmax}(\\theta)$ is the softmax function. The gradient of the logistic loss is then given by\n",
    "$$\\nabla l(\\theta) = -y + \\operatorname*{softmax}(\\theta).$$\n",
    "</div>\n",
    "\n",
    "### Linearity of gradients\n",
    "\n",
    "The notion of differentiability for multi-inputs functions naturally inherits from the linearity of derivatives for\n",
    "single-input functions. For any $u_1, \\dots, u_n\\in\\mathbb{R}$ and any multi-inputs functions $f_1, \\dots f_n$ that are\n",
    "differentiable at $x\\in\\mathbb{R}^n$, the function $f(x) = \\sum_{i=1}^n u_i f_i(x)$ is differentiable at $x$ with\n",
    "gradient $\\nabla f(x) = \\sum_{i=1}^n u_i \\nabla f_i(x).$ This property is a direct consequence of the linearity of the\n",
    "gradient.\n",
    "\n",
    "<div class=\"theorem\"><p>Theorem 1.2. Linearity of gradients</p>\n",
    "  Let $f_1, \\ldots, f_n:\\mathbb{R}^n\\to\\mathbb{R}$ be differentiable functions at $x\\in\\mathbb{R}^n$ and $u_1, \\ldots, u_n\\in\\mathbb{R}$. Then the function $f(x) = \\sum_{i=1}^n u_i f_i(x)$ is differentiable at $x$ with gradient $\\nabla f(x) = \\sum_{i=1}^n u_i \\nabla f_i(x).$\n",
    "</div>\n",
    "\n",
    "The gradient defines the steepest ascent direction of a function. To see why, notice that\n",
    "\n",
    "$$\\operatorname*{argmax}_{v\\in\\mathbb{R}^n, ||v||_2\\leq 1} \\partial f(x)[v] = \\operatorname*{argmax}_{v\\in\\mathbb{R}^n,\n",
    "||v||_2\\leq 1} \\langle v, \\nabla f(x)\\rangle = \\frac{\\nabla f(x)}{||\\nabla f(x)||_2},$$\n",
    "where we assumed that $\\nabla f(x)\\neq 0$ to avoid division by zero. The gradient $\\nabla f(x)$ is orthogonal to the\n",
    "level set of the function $f$ at $x$, meaning that the gradient points in the direction of the steepest ascent of $f$ at\n",
    "$x$. Conversely, the negative gradient $-\\nabla f(x)$ points in the direction of the steepest descent of $f$ at $x$.\n",
    "This observation motivates the development of optimization algorithms such as gradient descent, which iteratively\n",
    "updates the parameters $x$, $x_{n+1} = x_n - \\alpha \\nabla f(x_n)$, where $\\alpha > 0$ is a step size. It therefore\n",
    "seeks for the minimum of $f$ by following the steepest descent direction.\n",
    "\n",
    "### Jacobians\n",
    "\n",
    "Let us now consider a multi-output function $f:\\mathbb{R}^n\\to\\mathbb{R}^m$ defined by $f(x) = (f_1(x), \\ldots,\n",
    "f_m(x))$, where $f_i:\\mathbb{R}^n \\to \\mathbb{R}$ for $i = 1, \\ldots, m$. A typical example in machine learning is a\n",
    "neural network. The notion of directional derivative can be extended to such function by defining the **Jacobian\n",
    "matrix** of $f$ at $x$:\n",
    "\n",
    "$$\\partial f(x)[v] = \\lim_{h\\to 0} \\frac{f(x + hv) - f(x)}{h} = \\begin{pmatrix} \\partial f_1(x)[v] \\\\ \\vdots \\\\ \\partial\n",
    "f_m(x)[v] \\end{pmatrix} = \\begin{pmatrix} \\langle v, \\nabla f_1(x)\\rangle \\\\ \\vdots \\\\ \\langle v, \\nabla f_m(x)\\rangle\n",
    "\\end{pmatrix} = \\begin{pmatrix} \\nabla f_1(x)^\\top v \\\\ \\vdots \\\\ \\nabla f_m(x)^\\top v \\end{pmatrix} = \\nabla f(x)^\\top\n",
    "v.$$\n",
    "\n",
    "where the limits are applied coordinate-wise. The directional derivative of $f$ in the direction $v\\in\\mathbb{R}^n$ is\n",
    "therefore the vector that gathers the directional derivatives of each $f_j$, i.e., $\\nabla f(x)[v] = (\\partial\n",
    "f_i(v)[v])_{i=1}^m.$ The directional derivative of $f$ in the direction $v\\in \\mathbb{R}^n$ is therefore the vector that\n",
    "gathers the directional derivatives of each $f_i$, i.e., $\\nabla f(x)[v] = (\\partial f_i(v)[v])_{i=1}^m.$ In particular,\n",
    "we can define the **partial derivatives** of $f$ at $x$ as the vectors\n",
    "\n",
    "$$\\partial_i f(x) = \\begin{pmatrix} \\partial f_1(x)[e_i] \\\\ \\vdots \\\\ \\partial f_m(x)[e_i] \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\partial f_1(x)[e_i] \\\\\n",
    "\\vdots \\\\\n",
    "\\partial f_m(x)[e_i]\n",
    "\\end{pmatrix} = \\begin{pmatrix} \\nabla f_1(x)^\\top e_i \\\\ \\vdots \\\\ \\nabla f_m(x)^\\top e_i \\end{pmatrix} = \\begin{pmatrix} \\partial f_1(x)[e_i] \\\\ \\vdots \\\\ \\partial f_m(x)[e_i] \\end{pmatrix} = \\begin{pmatrix} \\partial f_1(x)[e_i] \\\\ \\vdots \\\\ \\partial f_m(x)[e_i] \\end{pmatrix} = \\begin{pmatrix} \\nabla f_1(x)^\\top e_i \\\\ \\vdots \\\\ \\nabla f_m(x)^\\top e_i \\end{pmatrix}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
